{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import pylab\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Bodies (20/1683):\n"
     ]
    }
   ],
   "source": [
    "trainBodies = pd.read_csv('fncData/train_bodies.csv')\n",
    "trainStances = pd.read_csv('fncData/train_stances.csv')\n",
    "trainStancesRandom = pd.read_csv('fncData/train_stances.random.csv')\n",
    "\n",
    "print \"Train Bodies (20/%s):\" % (len(trainBodies))\n",
    "# trainBodies.head(20)\n",
    "\n",
    "# to get articleBody:\n",
    "# trainBodies.get_value(0, 'articleBody')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Stances (20/49972):\n"
     ]
    }
   ],
   "source": [
    "print \"Train Stances (20/%s):\" % (len(trainStances))\n",
    "# trainStances.head(20)\n",
    "\n",
    "# trainStances.addColumn(stance === unrelated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Stances Random (20/49972)\n"
     ]
    }
   ],
   "source": [
    "print \"Train Stances Random (20/%s)\" % (len(trainStancesRandom))\n",
    "# trainStancesRandom.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('unrelated', 2), ('agree', 10), ('unrelated', 4), ('unrelated', 2), ('disagree', 6), ('agree', 14), ('unrelated', 0), ('unrelated', 2), ('agree', 2), ('unrelated', 1), ('discuss', 0), ('agree', 7), ('unrelated', 2), ('unrelated', 2), ('discuss', 4), ('unrelated', 7), ('discuss', 3), ('agree', 4), ('unrelated', 3), ('discuss', 2), (2, 'unrelated'), (10, 'agree'), (4, 'unrelated'), (2, 'unrelated'), (6, 'disagree'), (14, 'agree'), (0, 'unrelated'), (2, 'unrelated'), (2, 'agree'), (1, 'unrelated'), (0, 'discuss'), (7, 'agree'), (2, 'unrelated'), (2, 'unrelated'), (4, 'discuss'), (7, 'unrelated'), (3, 'discuss'), (4, 'agree'), (3, 'unrelated'), (2, 'discuss'), (2, 'unrelated'), (10, 'agree'), (4, 'unrelated'), (2, 'unrelated'), (6, 'disagree'), (14, 'agree'), (0, 'unrelated'), (2, 'unrelated'), (2, 'agree'), (1, 'unrelated'), (0, 'discuss'), (7, 'agree'), (2, 'unrelated'), (2, 'unrelated'), (4, 'discuss'), (7, 'unrelated'), (3, 'discuss'), (4, 'agree'), (3, 'unrelated'), (2, 'discuss'), (2, 'unrelated'), (10, 'agree'), (4, 'unrelated'), (2, 'unrelated'), (6, 'disagree'), (14, 'agree'), (0, 'unrelated'), (2, 'unrelated'), (2, 'agree'), (1, 'unrelated'), (0, 'discuss'), (7, 'agree'), (2, 'unrelated'), (2, 'unrelated'), (4, 'discuss'), (7, 'unrelated'), (3, 'discuss'), (4, 'agree'), (3, 'unrelated'), (2, 'discuss'), (2, 'unrelated'), (10, 'agree'), (4, 'unrelated'), (2, 'unrelated'), (6, 'disagree'), (14, 'agree'), (0, 'unrelated'), (2, 'unrelated'), (2, 'agree'), (1, 'unrelated'), (0, 'discuss'), (7, 'agree'), (2, 'unrelated'), (2, 'unrelated'), (4, 'discuss'), (7, 'unrelated'), (3, 'discuss'), (4, 'agree'), (3, 'unrelated'), (2, 'discuss'), (2, 'unrelated'), (10, 'agree'), (4, 'unrelated'), (2, 'unrelated'), (6, 'disagree'), (14, 'agree'), (0, 'unrelated'), (2, 'unrelated'), (2, 'agree'), (1, 'unrelated'), (0, 'discuss'), (7, 'agree'), (2, 'unrelated'), (2, 'unrelated'), (4, 'discuss'), (7, 'unrelated'), (3, 'discuss'), (4, 'agree'), (3, 'unrelated'), (2, 'discuss')]\n"
     ]
    }
   ],
   "source": [
    "# build feature vocabulary\n",
    "ngram = 1\n",
    "vectorizer = CountVectorizer(ngram_range=(1, ngram))\n",
    "\n",
    "testStances = trainStances[0:20]\n",
    "\n",
    "for index, row in testStances.iterrows():\n",
    "    headline = row['Headline']\n",
    "    bodyId = row['Body ID']\n",
    "    stance = row['Stance']\n",
    "    body = trainBodies[trainBodies['Body ID'] == bodyId]['articleBody'].item()\n",
    "    \n",
    "#     print index\n",
    "#     print \"bodyId: %s\\n\" % bodyId\n",
    "#     print \"HEADLINE: \\n%s\\n\" % headline\n",
    "#     print \"BODY: \\n%s\\n\" % body\n",
    "#     print \"STANCE: \\n%s\\n\" % stance\n",
    "\n",
    "    headline_words = headline.split()\n",
    "    body_words = body.split()\n",
    "    \n",
    "    num_words_in_common = len(set(headline_words).intersection(body_words))\n",
    "    \n",
    "    results.append((num_words_in_common, stance))\n",
    "\n",
    "print results \n",
    "\n",
    "# I'd like to graph the num_words_in_common integer to the stance classification, to see what kind of correlation we're working with here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
